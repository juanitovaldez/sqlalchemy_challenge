{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning and ORM:\n",
    "While hacking on my starting notebook, it became necessary to refactor and refocus the code. I will try and strictly limit the scope of this notebook to accomplish two tasks:\n",
    "1. Impute the missing null values\n",
    "2. Load just the imputed values into a table in the sqlite database, hawaii.sqlite\n",
    "\n",
    "## Coding Goals:\n",
    "I want to stick with sqlalchemy and its ORM to its fullest extent. Probe sqlite and it's features and functions. Idealy minimize pandas and ETL as much as possible.'\n",
    "\n",
    "## Run this notebook once and you should have another table with the imputed values added to the sqlite database.\n",
    "This might be handy if we want to see analyse the values our imputation process created. I've got a hunch that temperature alone is a poor predictor of precipitation values. Pressure might be better. Another step of the data cleaning process might include collecting more data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reserved for Libraries to import as I need them.\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "# for creating tables:\n",
    "from sqlalchemy import Table, TEXT, Column, FLOAT, ForeignKey, Integer\n",
    "# pandas for ETL\n",
    "import pandas as pd\n",
    "# sklearn for the impute\n",
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "database_path = \"Homework\\homework_08\\sqlalchemy_challenge\\data\\hawaii.sqlite\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_path = \"Homework\\homework_08\\sqlalchemy_challenge\\data\\hawaii.sqlite\"\n",
    "# First task is to create a session for my already existiung sqlite database I will need sqlalchemy create_engine, and sqlaclhemy.orm sessionmaker. I am going to need automap_base to create the classes from the database as it exists already. I did this manually in a sloppy manner in the previous notbook. # There is an order to doing this that I do not quite understand yet. I Declare my base and then create my engine. OK actually I have to pre-declare my new table. I'll call it \"imputed\"\n",
    "Base = automap_base()\n",
    "# This is the same setup as in 'measurement' table. Except for the relationships. There is a more sqlalchemy way to declare this table though...\n",
    "class Imputed(Base):\n",
    "    __tablename__ = 'imputed'\n",
    "   # __table_args__ = ''\n",
    "\n",
    "    id = Column(Integer, primary_key = True)\n",
    "    station = Column('station', TEXT())\n",
    "    date = Column('date', TEXT())\n",
    "    prcp = Column('prcp', FLOAT())\n",
    "    tobs = Column('tobs', FLOAT())\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'station={self.station}\\\n",
    "                date={self.date}\\\n",
    "                prcp{self.prcp}\\\n",
    "                tobs={self.tobs}'\n",
    "\n",
    "engine = create_engine(f'sqlite:///{database_path}',  connect_args={'timeout': 15})\n",
    "# Create our table?\n",
    "Base.metadata.create_all(engine)\n",
    "# Now to do the reflect and whatnot\n",
    "Base = automap_base()\n",
    "Base.prepare(engine, reflect=True)\n",
    "Measurement = Base.classes.measurement\n",
    "Station = Base.classes.station\n",
    "Imputed = Base.classes.imputed\n",
    "# Now to make a session for our queries:\n",
    "conn = engine.engine.connect()\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Cleaning:\n",
    "Extract whole record => Impute Null Values => Populate imputed table with the values from previous step"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "measurement_df = pd.read_sql_table('measurement', conn)\n",
    "null_impute_df = pd.read_sql_query('SELECT * FROM measurement WHERE prcp ISNULL;', conn, index_col='id')\n",
    "conn.close()\n",
    "null_impute_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will take a dataframe and spit out a dataframe with the values imputed:\n",
    "def impute_prcp(data_frame):\n",
    "    impute_fit = data_frame.sample(int(len(measurement_df)/3))\n",
    "    imp = IterativeImputer(max_iter=1000, random_state=1235312395)\n",
    "    imp.fit(impute_fit[['prcp','tobs']])\n",
    "    impute_df =  pd.DataFrame(np.round(imp.transform(data_frame[['prcp', 'tobs']]),2), columns = ['prcp', 'tobs'])\n",
    "    cat_df = data_frame\n",
    "    cat_df['prcp_imp'] = impute_df['prcp']\n",
    "    return cat_df"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some quick hacking to get the dataframe into the right format: \n",
    "test = impute_prcp(measurement_df)\n",
    "cols = ['id', 'station','date', 'prcp', 'tobs']\n",
    "merge_df = pd.merge(null_impute_df, test, how='inner', sort=False, on=['id', 'station', 'date', 'tobs'])\n",
    "imputed_df = merge_df[['id', 'station','date', 'prcp_imp', 'tobs']]\n",
    "imputed_df['prcp'] = imputed_df['prcp_imp']\n",
    "imputed_df = imputed_df[cols]\n",
    "imputed_df = imputed_df.set_index('id')\n",
    "imputed_df\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bulk insert was giving me concurrency errors that I couldn't figure out. Sqlite plays nice with pandas to_sql, so let's just do that.\n",
    "imputed_df.to_sql('imputed', engine, if_exists='append', index='id')\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}